{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c624413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from rtdip_sdk.pipelines.execute import PipelineJob, PipelineStep, PipelineTask, PipelineJobExecute\n",
    "from src.rtdip_sources import OpenMeteoHistoricalWeatherSource, MultiFileSource\n",
    "from src.rtdip_destinations import DiskDestination\n",
    "from src.rtdip_transformations import FunctionTransformer\n",
    "\n",
    "from src.feature_extraction import prepare_dataset\n",
    "from src.training import train\n",
    "\n",
    "from src.settings import DATA_DIR, LAT_COL, LON_COL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009bd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scroodge_meta = pd.read_parquet(DATA_DIR / \"scrooge_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da04a7",
   "metadata": {},
   "source": [
    "# Gather external data\n",
    "\n",
    "The only external data I used were weather data from the Open Meteo API.  \n",
    "We can use a RTDIP Job to download the data and store them in a file.  \n",
    "Check out the OpenMeteoHistoricalWeatherSource and DiskDestination components I have developed for this purpose.\n",
    "\n",
    "Let's now define the pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7556dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_variables = [\"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"wind_direction_10m\",\n",
    "                     \"direct_radiation_instant\", \"diffuse_radiation_instant\", \"direct_normal_irradiance_instant\",\n",
    "                     \"global_tilted_irradiance_instant\"]\n",
    "latitude = scroodge_meta[LAT_COL].values[0]\n",
    "longitude = scroodge_meta[LON_COL].values[0]\n",
    "start_date = \"2017-12-31\"\n",
    "end_date = \"2019-01-01\"\n",
    "timezone = \"America/New_York\"\n",
    "\n",
    "get_weather_data_step = PipelineStep(\n",
    "    name=\"get_weather_data\",\n",
    "    description=\"Download weather data from Open Meteo\",\n",
    "    component=OpenMeteoHistoricalWeatherSource,\n",
    "    component_parameters={\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"variables\": weather_variables,\n",
    "        \"timezone\": timezone,\n",
    "        \"data_frequency\": \"hourly\"\n",
    "    },\n",
    "    provide_output_to_step=[\"dump_weather_data\"]\n",
    ")\n",
    "\n",
    "dump_weather_data_step = PipelineStep(\n",
    "    name=\"dump_weather_data\",\n",
    "    description=\"Store weather data to disk\",\n",
    "    component=DiskDestination,\n",
    "    component_parameters={\n",
    "        \"path\": DATA_DIR / \"scrooge_weather.parquet\",\n",
    "        \"file_type\": \"parquet\"\n",
    "    },\n",
    "    depends_on_step=[\"get_weather_data\"]\n",
    ")\n",
    "\n",
    "external_data_steps = [get_weather_data_step, dump_weather_data_step]\n",
    "\n",
    "external_data_task = PipelineTask(\n",
    "    name=\"external_data\",\n",
    "    description=\"Fetch external data\",\n",
    "    step_list=external_data_steps,\n",
    "    batch_task=True\n",
    ")\n",
    "\n",
    "pipeline_job = PipelineJob(\n",
    "    name=\"test_job\",\n",
    "    description=\"test_job\", \n",
    "    version=\"0.0.1\",\n",
    "    task_list=[external_data_task]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ed5031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather_data\n",
      "dump_weather_data\n"
     ]
    }
   ],
   "source": [
    "pipeline = PipelineJobExecute(pipeline_job)\n",
    "\n",
    "result = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42016d56",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "For the modeling part I adopted a two step model.  \n",
    "The first step deals with the temperature dependence of the electricity consumption and fits a linear model with a kink.  \n",
    "After the kink the consumption is zero.  \n",
    "The second model takes as an input the residuals of the first and fits a LightGBM regressor to them.  \n",
    "\n",
    "The model is implemented in the `ElectricityModel` class in `src/model.py`  \n",
    "\n",
    "In order to run the training step using RTDIP I developed additional sources and transformers wrapping custom python functions:\n",
    "- `MultiFileSource` : this one loads a list of files. It remedies the fact that there is no joining possibility within the RTDIP components.\n",
    "- `FunctionTransformer` : this one wraps a custom python function.\n",
    "\n",
    "So, let's define our training pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aaea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrooge_electricity_path = DATA_DIR / \"scrooge_bldg.parquet\"\n",
    "scrooge_weather_path = DATA_DIR / \"scrooge_weather.parquet\"\n",
    "scrooge_meta_path = DATA_DIR / \"scrooge_metadata.parquet\"\n",
    "\n",
    "load_raw_data_step = PipelineStep(\n",
    "    name=\"load_raw_data\",\n",
    "    description=\"Load raw data\",\n",
    "    component=MultiFileSource,\n",
    "    component_parameters={\n",
    "        \"file_confs\": [\n",
    "            (str(scrooge_weather_path), \"parquet\"),\n",
    "            (str(scrooge_electricity_path), \"parquet\"),\n",
    "            (str(scrooge_meta_path), \"parquet\")\n",
    "        ],\n",
    "    },\n",
    "    provide_output_to_step=[\"feature_extraction\"]\n",
    ")\n",
    "\n",
    "\n",
    "feature_extraction_step = PipelineStep(\n",
    "    name=\"feature_extraction\",\n",
    "    description=\"Extract features\",\n",
    "    component=FunctionTransformer,\n",
    "    component_parameters={\n",
    "        \"function\": prepare_dataset,\n",
    "    },\n",
    "    provide_output_to_step=[\"dump_features\", \"training\"],\n",
    "    depends_on_step=[\"load_raw_data\"]\n",
    ")\n",
    "\n",
    "dump_features_step = PipelineStep(\n",
    "    name=\"dump_features\",\n",
    "    description=\"Store weather data to disk\",\n",
    "    component=DiskDestination,\n",
    "    component_parameters={\n",
    "        \"path\": DATA_DIR / \"features.parquet\",\n",
    "        \"file_type\": \"parquet\"\n",
    "    },\n",
    "    depends_on_step=[\"feature_extraction\"]\n",
    ")\n",
    "\n",
    "training_step = PipelineStep(\n",
    "    name=\"training\",\n",
    "    description=\"Train model\",\n",
    "    component=FunctionTransformer,\n",
    "    component_parameters={\n",
    "        \"function\": train,\n",
    "    },\n",
    "    provide_output_to_step=[\"dump_model\"],\n",
    "    depends_on_step=[\"feature_extraction\"]\n",
    ")\n",
    "\n",
    "dump_model_step = PipelineStep(\n",
    "    name=\"dump_model\",\n",
    "    description=\"Store model to disk\",\n",
    "    component=DiskDestination,\n",
    "    component_parameters={\n",
    "        \"path\": DATA_DIR / \"models.pickle\",\n",
    "        \"file_type\": \"pickle\"\n",
    "    },\n",
    "    depends_on_step=[\"training\"]\n",
    ")\n",
    "\n",
    "modeling_steps = [load_raw_data_step, feature_extraction_step, dump_features_step, training_step, dump_model_step]\n",
    "\n",
    "modeling_task = PipelineTask(\n",
    "    name=\"modeling\",\n",
    "    description=\"Modeling\",\n",
    "    step_list=modeling_steps,\n",
    "    batch_task=True\n",
    ")\n",
    "\n",
    "modeling_job = PipelineJob(\n",
    "    name=\"test_job\",\n",
    "    description=\"test_job\", \n",
    "    version=\"0.0.1\",\n",
    "    task_list=[modeling_task]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6af83f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_raw_data\n",
      "feature_extraction\n",
      "Finished feature extraction\n",
      "Index(['timestamp', 'out.electricity.heating_hp_bkup.energy_consumption',\n",
      "       'out.electricity.heating.energy_consumption',\n",
      "       'out.electricity.plug_loads.energy_consumption', 'temperature_2m',\n",
      "       'relative_humidity_2m', 'wind_speed_10m', 'wind_direction_10m',\n",
      "       'direct_radiation_instant', 'diffuse_radiation_instant',\n",
      "       ...\n",
      "       'temperature_setpoint_ratio_diff_ratio_over_previous_lag3_offset2',\n",
      "       'decimal_hour_cat', 'month_cat', 'dayofweek_cat', 'temperature_cat',\n",
      "       'holiday', 'days_from_holiday', 'days_to_holiday', 'heating_total',\n",
      "       'heating_and_plugs'],\n",
      "      dtype='object', length=428)\n",
      "['bldg_id']\n",
      "dump_features\n",
      "training\n",
      "Starting training\n",
      "Number of features: 28\n",
      "Fitting model for variable: heating_total\n",
      "Fitting model for variable: out.electricity.plug_loads.energy_consumption\n",
      "dump_model\n"
     ]
    }
   ],
   "source": [
    "# execute the modeling job\n",
    "\n",
    "pipeline = PipelineJobExecute(modeling_job)\n",
    "result = pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8b1ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction\n",
      "Index(['timestamp', 'out.electricity.heating_hp_bkup.energy_consumption',\n",
      "       'out.electricity.heating.energy_consumption',\n",
      "       'out.electricity.plug_loads.energy_consumption', 'temperature_2m',\n",
      "       'relative_humidity_2m', 'wind_speed_10m', 'wind_direction_10m',\n",
      "       'direct_radiation_instant', 'diffuse_radiation_instant',\n",
      "       ...\n",
      "       'temperature_setpoint_ratio_diff_ratio_over_previous_lag3_offset2',\n",
      "       'decimal_hour_cat', 'month_cat', 'dayofweek_cat', 'temperature_cat',\n",
      "       'holiday', 'days_from_holiday', 'days_to_holiday', 'heating_total',\n",
      "       'heating_and_plugs'],\n",
      "      dtype='object', length=428)\n",
      "['bldg_id']\n",
      "['bldg_id']\n",
      "heating_total                                                              float64\n",
      "out.electricity.plug_loads.energy_consumption                              float64\n",
      "temperature_2m_lag12                                                       float32\n",
      "wind_speed_10m_rolling_mean_lag1_window_size4                              float32\n",
      "wind_speed_10m_rolling_mean_lag1_window_size8                              float32\n",
      "minute                                                                       int32\n",
      "wind_speed_10m_diff_over_previous_lag1                                     float32\n",
      "temperature_2m                                                             float32\n",
      "dayofweek                                                                    int32\n",
      "bldg_id                                                                     object\n",
      "days_from_holiday                                                            int64\n",
      "days_to_holiday                                                            float64\n",
      "is_weekend                                                                    bool\n",
      "holiday                                                                       bool\n",
      "temperature_2m_lag8                                                        float32\n",
      "hour                                                                         int32\n",
      "decimal_hour                                                               float64\n",
      "timestamp                                                           datetime64[ns]\n",
      "global_tilted_irradiance_instant_rolling_trend_lag1_window_size8           float32\n",
      "global_tilted_irradiance_instant_lag10                                     float32\n",
      "wind_direction_10m_lag10                                                   float32\n",
      "time_to_sunrise                                                            float64\n",
      "wind_direction_10m_rolling_mean_lag1_window_size8                          float32\n",
      "direct_radiation_instant_rolling_trend_lag1_window_size8                   float32\n",
      "wind_speed_10m_diff_over_previous_lag1_offset2                             float32\n",
      "decimal_hour_cat                                                          category\n",
      "temperature_setpoint_ratio_diff_over_previous_lag1                         float32\n",
      "relative_humidity_2m_rolling_trend_lag1_window_size4                       float32\n",
      "dtype: object\n",
      "                timestamp  temperature_2m\n",
      "34737 2018-12-28 20:15:00       10.756000\n",
      "34738 2018-12-28 20:30:00       10.843500\n",
      "34739 2018-12-28 20:45:00       10.931000\n",
      "34740 2018-12-28 21:00:00       11.018499\n",
      "34741 2018-12-28 21:15:00       11.168499\n",
      "34742 2018-12-28 21:30:00       11.318500\n",
      "34743 2018-12-28 21:45:00       11.468500\n",
      "34744 2018-12-28 22:00:00       11.618500\n",
      "34745 2018-12-28 22:15:00       11.781000\n",
      "34746 2018-12-28 22:30:00       11.943500\n",
      "34747 2018-12-28 22:45:00       12.105999\n",
      "34748 2018-12-28 23:00:00       12.268499\n",
      "34749 2018-12-28 23:15:00       12.193500\n",
      "34750 2018-12-28 23:30:00       12.118500\n",
      "34751 2018-12-28 23:45:00       12.043499\n",
      "34752 2018-12-29 00:00:00       11.968499\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>party_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-28 20:15:00</td>\n",
       "      <td>0.221412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-28 20:30:00</td>\n",
       "      <td>0.214707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-28 20:45:00</td>\n",
       "      <td>0.207453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-28 21:00:00</td>\n",
       "      <td>0.206058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-28 21:15:00</td>\n",
       "      <td>0.195291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-12-28 21:30:00</td>\n",
       "      <td>0.193309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-12-28 21:45:00</td>\n",
       "      <td>0.195087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-12-28 22:00:00</td>\n",
       "      <td>0.197503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-12-28 22:15:00</td>\n",
       "      <td>0.111531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-12-28 22:30:00</td>\n",
       "      <td>0.113221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-12-28 22:45:00</td>\n",
       "      <td>0.114864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-12-28 23:00:00</td>\n",
       "      <td>0.115054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-12-28 23:15:00</td>\n",
       "      <td>0.113744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-12-28 23:30:00</td>\n",
       "      <td>0.117053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-12-28 23:45:00</td>\n",
       "      <td>0.115420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-12-29 00:00:00</td>\n",
       "      <td>0.114101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp  party_cost\n",
       "0   2018-12-28 20:15:00    0.221412\n",
       "1   2018-12-28 20:30:00    0.214707\n",
       "2   2018-12-28 20:45:00    0.207453\n",
       "3   2018-12-28 21:00:00    0.206058\n",
       "4   2018-12-28 21:15:00    0.195291\n",
       "5   2018-12-28 21:30:00    0.193309\n",
       "6   2018-12-28 21:45:00    0.195087\n",
       "7   2018-12-28 22:00:00    0.197503\n",
       "8   2018-12-28 22:15:00    0.111531\n",
       "9   2018-12-28 22:30:00    0.113221\n",
       "10  2018-12-28 22:45:00    0.114864\n",
       "11  2018-12-28 23:00:00    0.115054\n",
       "12  2018-12-28 23:15:00    0.113744\n",
       "13  2018-12-28 23:30:00    0.117053\n",
       "14  2018-12-28 23:45:00    0.115420\n",
       "15  2018-12-29 00:00:00    0.114101"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from src.prepare_submission import prepare\n",
    "\n",
    "with open(DATA_DIR / \"models.pickle\", \"rb\") as fh:\n",
    "    models = pickle.load(fh)\n",
    "\n",
    "dataset = pd.read_parquet(DATA_DIR/\"features.parquet\")\n",
    "\n",
    "prepare(models)\n",
    "\n",
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
