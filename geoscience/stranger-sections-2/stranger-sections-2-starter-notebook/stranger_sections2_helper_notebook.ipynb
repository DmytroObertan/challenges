{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stranger Sections 2 - Helper Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for participating the the Stranger Sections 2 Challenge! This challenge aims for participants to build a model that is capable of segmenting kerogen macerals on reflected light polished sections. The goal of this helper notebook is to spark ideas for unique ways to solve this challenge. You do not need to be a geoscientist to be successful in this challenge. In fact, unique approaches from other disciplines might be just what we need to solve the problem! Please familiarize yourself with the challenge description, evaluation criteria, and data size and type before diving into this helper notebook. You can find all relevant information on the [challenge page](https://thinkonward.com/app/c/challenges/stranger-sections-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You did what?!** For this challenge we realeased a LOT of data. 8,728 images of reflected light polished sections to be exact. Of these images, 87 have segmentation labels. Given that most of the data is unlabeled, what are some creative ways to use this data. We don't assume or expect challengers to be organic petrologists, so labeling by hand seems out of the question. One method the Onward team has been exploring is Vision Transformers! What happens if we further pretrain an existing ViT backbone with unlabeled thin section images?! Can we use our fine-tuning module to get improved segmentation results over an out of the box method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the starter notebook we provided code that gives you the ability to further pre-train a Vision Transformer backbone using the unlabeled images from the challenge. The pre-training code can be found in the `masked_autoencoder.pretrain` module. The pretraining script can be run in Jupyter or terminal. Please see the the starter notebook for instructions to run in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"masked_autoencoder\")\n",
    "from masked_autoencoder.pretrain import pretrain_mae\n",
    "from utils import plot_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in your dataset catalog should contain 3D numpy arrays stored as `.npy` files. We recommend to start with data from Stranger Sections 2 challange.\n",
    "dataset = \"./data/unlabeled_images/\"\n",
    "\n",
    "# We will start with default torchvision checkpoint - pretrained on ImageNet1k data\n",
    "history = pretrain_mae(\n",
    "    dataset,  # your checkpoint will be saved to this location - catalog must exist before training!\n",
    "    output=\"checkpoints/ViT_B_16_stranger_sections2.pth\",\n",
    "    transform_kwargs={\n",
    "        \"min_scale\": 0.33,\n",
    "        \"normalize\": False,\n",
    "    },  # this will be passed to lightly.transforms.mae_transform.MAETransform\n",
    "    vit_model=\"ViT_B_16\",\n",
    "    starting_weights=\"ViT_B_16_Weights.DEFAULT\",\n",
    "    batch_size=64,\n",
    "    n_workers=8,  # to achieve best performance we recommend setting this value to match number of physical cores of your CPU\n",
    "    optimizer=\"SGD\",  # AdamW and SGD are supported out of the box\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 1e-4,\n",
    "        \"momentum\": 0.0,\n",
    "    },  # this will be passed to optimizer constructor\n",
    "    warmup_epochs=20,  # tune Learning Rate schedules to match your needs\n",
    "    start_factor=0.2,  # starting decrease factor for warmup\n",
    "    linear_schedule=True,\n",
    "    end_factor=0.5,  # final decrease factor for LR schedule\n",
    "    n_epochs=200,  # number of non-warmup training epochs\n",
    "    masking_rate=0.75,  # define what share of input image will be masked\n",
    "    decoder_dim=256,  # we will use smaller decoder as we're training the smallest model of ViT family\n",
    "    freeze_projection=False, # we will start with trainig whole model\n",
    "    freeze_embeddings=False,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_report(history, x_locator_tick=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation Fine-tuneing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
